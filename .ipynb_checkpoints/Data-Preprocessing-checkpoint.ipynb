{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e2360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e474e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a csv using the pandas library, saving in a 'variable'\n",
    "# function will read the dataset and create a dataframe (the dataset variable)\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "# Creating 2 entities,the first is the matrix of features and the second is the dependent variable vector \n",
    "# In any machine learning model that we'll build, the features usually go in the first columns in the dataset, and the dependent variable in the last\n",
    "# In the dataset example imported from the Data.csv file, our matrix of features includes columns 'Country', 'Age' and 'Salary'\n",
    "# Seperately in the imported example, we want to create the dependent variable vector containing only the last column 'Purchased'\n",
    "# The 'Purchased' column, representing a boolean is the one we want to predict in this example\n",
    "\n",
    "# The Matrix of features\n",
    "# using iloc (locate rows/columns indexes), that located the indexes that we want to extract from the dataset\n",
    "# rows first with iloc, here adding ':' to specify all rows\n",
    "# including all the columns except the last one -> The matrix of features -> ':-1' this will exclude the last column\n",
    "# at the end, using '.values' that indicates that we are taking the values in all the rows and columns of the dataset, except last column\n",
    "X = dataset.iloc[:, :-1].values\n",
    "\n",
    "# The dependent variable vector\n",
    "y = dataset.iloc[:, -1].values # selecting only last column as dependent variable vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e431232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Contains all the features, except the last one that we want to predict\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246401b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Contains all the decisions of whether or not the customer purchased the product\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2632ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8ed39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a67deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Taking care of missing data -> Causes errors in ML model\n",
    "# One way is to remove them, another is to replace with data average from same feature\n",
    "\n",
    "# sklearn data science library includes a lot of tools, including a lot of data pre-processing tools\n",
    "# will use the class 'simple imputer' from sklearn\n",
    "# Importing the simple imputer class\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating an instance(an object) of the simple imputer class, that will allow us to replace the missing salary in the dataset with the average of the salries\n",
    "# This will allow us to have an updated dataset (matrix of features)\n",
    "# Note: Options for replacing missing values other than with Average, can be median, most frequent value (relevant for categories)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Applying (connecting) the impute object on the matrix of features\n",
    "# Remember a class contains an assemble of instructions as well as operations and actions that can be applied to other objects called methods\n",
    "# The fit method will connect this imputer to the matrix of features\n",
    "# The fit method will look at the missing values in the salary column and will compute the average of salaries\n",
    "# To do the replacement we call transform method which will apply the transformation by replacing the missing salary with the average of the salaries\n",
    "\n",
    "# The fit method expects all the columns of 'X' with numerical values\n",
    "# As a general rule include all numerical columns, as in huge datasets, it s hard to spot which have missing values\n",
    "imputer.fit(X[:, 1:3]) # fit method will look for all missing values in the age and salary columns\n",
    "\n",
    "# Transform method from imputer object\n",
    "# This will do the actual replacement of missing salary be mean of salaries (same thing for missing age)\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # this returns the new updated version of the matrix of features 'X' with the replacements of the missing salary and age for that we are directly replacing them\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3f35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcad31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0f55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# In our dataset, there is a column with a country category (France, Spain or Germany)\n",
    "# For ML model, it would be difficult to compute some correlations between these features and the outcome (dependent variable)\n",
    "# We need to turn the categories (strings) into numbers\n",
    "# An idea would be to encode France into '0', Spain into '1' and Germany into '2'\n",
    "# However, if we did that, our future ML model can understand that because they are numbered 0, 1 and 2 then there is a numerical order between these countries\n",
    "# The ML model might interpret that the numerical order matters in such a case, when in fact it does not\n",
    "# To avoid such mis-interpreted correlations between the features and the outcome we want to predict\n",
    "\n",
    "# What we can do is turn country column into 3 columns in this case as we have 3 different classes (categories)\n",
    "# This consists of creating binary vectors for each of the countries\n",
    "# e.g. France will have the vector [1,0,0], Spain [0,1,0] and Germany [0,0,1]\n",
    "# This way there's no numerical order between these 3 countries\n",
    "# This is called one-hut encoding, very useful when pre-processing datasets containing categorical variables\n",
    "\n",
    "# no and yes in the Purchased column will be replaced by zeros and ones\n",
    "\n",
    "# Using 2 classes, first is column transformer class from the compose module of the sklearn library\n",
    "# Second is the one-hut encoder class from the pre-processing module of the same sklearn library\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Mixing those 2 classes in order to do the one hot encoding on the country column\n",
    "\n",
    "# Step 1: Creating an object of the column transformer class\n",
    "# 2 arguments go into ColumnTransformer:\n",
    "# Transformers -> where we specify what kind of transformations we want to do and on which indexes of the columns we want to transform\n",
    "# Remainder -> Specifying the columns that will not have transformations applied to (here age and salary)\n",
    "ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0])] ,remainder='passthrough')\n",
    "\n",
    "# Connecting to matrix of feature X, this can be done directly because column transformer class has a method 'fit_transform' that will fit and transform at once\n",
    "# Forcing the output to be a numPy array -> the fit_transform method does not return a numPy array, and its compulsory to have X as a numPy array\n",
    "# This is because it will be expected by future ML models that will be built\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
