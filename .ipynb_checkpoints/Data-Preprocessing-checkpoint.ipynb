{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60689ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64afbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a csv using the pandas library, saving in a 'variable'\n",
    "# function will read the dataset and create a dataframe (the dataset variable)\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "# Creating 2 entities,the first is the matrix of features and the second is the dependent variable vector \n",
    "# In any machine learning model that we'll build, the features usually go in the first columns in the dataset, and the dependent variable in the last\n",
    "# In the dataset example imported from the Data.csv file, our matrix of features includes columns 'Country', 'Age' and 'Salary'\n",
    "# Seperately in the imported example, we want to create the dependent variable vector containing only the last column 'Purchased'\n",
    "# The 'Purchased' column, representing a boolean is the one we want to predict in this example\n",
    "\n",
    "# The Matrix of features\n",
    "# using iloc (locate rows/columns indexes), that located the indexes that we want to extract from the dataset\n",
    "# rows first with iloc, here adding ':' to specify all rows\n",
    "# including all the columns except the last one -> The matrix of features -> ':-1' this will exclude the last column\n",
    "# at the end, using '.values' that indicates that we are taking the values in all the rows and columns of the dataset, except last column\n",
    "X = dataset.iloc[:, :-1].values\n",
    "\n",
    "# The dependent variable vector\n",
    "y = dataset.iloc[:, -1].values # selecting only last column as dependent variable vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78e3600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Contains all the features, except the last one that we want to predict\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35917cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Contains all the decisions of whether or not the customer purchased the product\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd80a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308f59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e999efe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Taking care of missing data -> Causes errors in ML model\n",
    "# One way is to remove them, another is to replace with data average from same feature\n",
    "\n",
    "# sklearn data science library includes a lot of tools, including a lot of data pre-processing tools\n",
    "# will use the class 'simple imputer' from sklearn\n",
    "# Importing the simple imputer class\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating an instance(an object) of the simple imputer class, that will allow us to replace the missing salary in the dataset with the average of the salries\n",
    "# This will allow us to have an updated dataset (matrix of features)\n",
    "# Note: Options for replacing missing values other than with Average, can be median, most frequent value (relevant for categories)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Applying (connecting) the impute object on the matrix of features\n",
    "# Remember a class contains an assemble of instructions as well as operations and actions that can be applied to other objects called methods\n",
    "# The fit method will connect this imputer to the matrix of features\n",
    "# The fit method will look at the missing values in the salary column and will compute the average of salaries\n",
    "# To do the replacement we call transform method which will apply the transformation by replacing the missing salary with the average of the salaries\n",
    "\n",
    "# The fit method expects all the columns of 'X' with numerical values\n",
    "# As a general rule include all numerical columns, as in huge datasets, it s hard to spot which have missing values\n",
    "imputer.fit(X[:, 1:3]) # fit method will look for all missing values in the age and salary columns\n",
    "\n",
    "# Transform method from imputer object\n",
    "# This will do the actual replacement of missing salary be mean of salaries (same thing for missing age)\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # this returns the new updated version of the matrix of features 'X' with the replacements of the missing salary and age for that we are directly replacing them\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cceaef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f89c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c221f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# In our dataset, there is a column with a country category (France, Spain or Germany)\n",
    "# For ML model, it would be difficult to compute some correlations between these features and the outcome (dependent variable)\n",
    "# We need to turn the categories (strings) into numbers\n",
    "# An idea would be to encode France into '0', Spain into '1' and Germany into '2'\n",
    "# However, if we did that, our future ML model can understand that because they are numbered 0, 1 and 2 then there is a numerical order between these countries\n",
    "# The ML model might interpret that the numerical order matters in such a case, when in fact it does not\n",
    "# To avoid such mis-interpreted correlations between the features and the outcome we want to predict\n",
    "\n",
    "# What we can do is turn country column into 3 columns in this case as we have 3 different classes (categories)\n",
    "# This consists of creating binary vectors for each of the countries\n",
    "# e.g. France will have the vector [1,0,0], Spain [0,1,0] and Germany [0,0,1]\n",
    "# This way there's no numerical order between these 3 countries\n",
    "# This is called one-hut encoding, very useful when pre-processing datasets containing categorical variables\n",
    "\n",
    "# no and yes in the Purchased column will be replaced by zeros and ones\n",
    "\n",
    "# Using 2 classes, first is column transformer class from the compose module of the sklearn library\n",
    "# Second is the one-hut encoder class from the pre-processing module of the same sklearn library\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Mixing those 2 classes in order to do the one hot encoding on the country column\n",
    "\n",
    "# Step 1: Creating an object of the column transformer class\n",
    "# 2 arguments go into ColumnTransformer:\n",
    "# Transformers -> where we specify what kind of transformations we want to do and on which indexes of the columns we want to transform\n",
    "# Remainder -> Specifying the columns that will not have transformations applied to (here age and salary)\n",
    "ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0])] ,remainder='passthrough')\n",
    "\n",
    "# Step 2: Connecting to matrix of feature X, this can be done directly because column transformer class has a method 'fit_transform' that will fit and transform at once\n",
    "# Forcing the output to be a numPy array -> the fit_transform method does not return a numPy array, and its compulsory to have X as a numPy array\n",
    "# This is because it will be expected by future ML models that will be built\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38042f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bbef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e33d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the Dependent Variable (Purchased) -> converting 'No' and 'Yes' strings into 0 and 1\n",
    "# Using Label encoder class\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y) # This doesn't need to be forced into a np array as its the dependent variable vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d51007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346455e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ead4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3537e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55aa769f",
   "metadata": {},
   "source": [
    "### Splitting the data into Training Set and Test Set. After that comes Feature Scaling, not before!\n",
    "\n",
    "### REASON:\n",
    "\n",
    "      The test set is supposed to be a brand new set, on which we will evaluate our ML model\n",
    "      \n",
    "      Meaning we're not supposed to work with the test set for the training\n",
    "      \n",
    "      Feature Scaling is a technique that will get the mean and standard deviation of our feature in order to perform the scaling\n",
    "      \n",
    "      So if we apply feature scaling before the split then it will get the mean and std of all the values including those in the test set -> Information leakage -> BAD\n",
    "      \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1ce281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and test sets using a model called model selection from sklearn that contains a function called train test split\n",
    "# Will create 4 seperate sets (not 2)\n",
    "#  this is because we will create a pair of matrix of features independent variable for the training set (X train, y train)\n",
    "#  and another pair of matrix of features independent variable for the test set (X test and y test)\n",
    "\n",
    "# The future ML model will be all of them expecting this format as inputs\n",
    "# for training using fit method, the ML model will expect X train and y train\n",
    "# for the predictions, also called inference, these ML models will predict X test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# since we already know what the train_test_split function returns, creating the variables here\n",
    "# X_train: the matrix of features of the training set (containing all the countries one hot encoded, the age and the salaries)\n",
    "# X_test: the matric of frautures of the test set\n",
    "# y_train: the dependent variable of the training set (all the purchased decisions of the customers in the training set)\n",
    "# y-test:  the dependant variable of the test set\n",
    "# specifying split size (a lot in training observations in training set and fewer in test) -> recommended 80-20% split\n",
    "# Since there are random factors that will happen during the split, we need to ensure that the same random factors we add random_state =1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd69e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469d816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0af4691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b78b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd38f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62873817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e7302c9",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "### Feature scaling allows us to put all our features on the same scale\n",
    "\n",
    "### Why? for some ML models its in order to avoid some features dominating other features, in a way that the dominated features are not even considered by the ML model\n",
    "\n",
    "### This is not necessary for all ML models\n",
    "\n",
    "## The main 2 feature scaling techniques that put our features in the same scale:\n",
    "\n",
    "# - Standardisation:\n",
    "\n",
    "    consists of substracting each value of our feature by the mean of all the values of the features and then dividing by the standard deviation (the square root of the variance)\n",
    "    \n",
    "    => This will put all the values of the feature between -3 and +3 (more or less) for all different features we apply standardisation to\n",
    "    \n",
    "\n",
    "### > xstand = (x - mean(x)) / (standard deviation (x))\n",
    "\n",
    "### Standardisation works well all the time, not just when there is a normal distribution -> Recommended to always use Standardisation for feature scaling as it works well with all distributions\n",
    "\n",
    "\n",
    "\n",
    "# - Normalisation:\n",
    "    \n",
    "    consists of substracting each value of the feature by the minimum value of the feature, then dividing be the difference between the maximum and minimum values of the feature\n",
    "    \n",
    "    => All the values of the features will become between 0 and 1\n",
    "\n",
    "    \n",
    "\n",
    "### > xnorm = (x - min(x)) / (max(x)-min(x))\n",
    "\n",
    "### Normalisation is recommended when we have a normal distribution in most of our features -> In that case Normalisation would shine as a feature scaling technique\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing feature scaling, using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creating an instance of the StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Remember! only apply feature scaling on numerical values\n",
    "\n",
    "# fitting our standardisation tool on the training set\n",
    "# we will not apply feature scaling on the dummy variable, thus taking only 2 columns(age and salary) with all rows\n",
    "\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) #this will take the age and salary columns, the fit method will compute the mean and standard deviation. After fit, tansform will transorm the values to be on the same scale\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
